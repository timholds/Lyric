{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from nltk import word_tokenize\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import colors\n",
    "import numpy as np\n",
    "\n",
    "#nltk.download('cmudict')\n",
    "arpabet = nltk.corpus.cmudict.dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "code_folding": [
     0,
     1
    ],
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_slang_dict_dirty():\n",
    "    slang = {\n",
    "        \"prob'ly\":['P R AA1', 'B L IY0'],\n",
    "          \"practic'ly\": ['P R AE1 K','T AH0 K','L IY0'],\n",
    "          \"acquiescent\":  [\"AE2\",\"K W IY0\",\"EH1\",\"S AH0 N T\"],\n",
    "          \"mediocrities\": [\"M IY2\",\"D IY0\",\"AA1\",\"K R AH0\",\"T IY0 Z\"],\n",
    "          \"casse\": [\"K AH0 S\"],\n",
    "          \"bonsoir\": [\"B AA0 N\",\"S W AA1\"],\n",
    "          \"m'appelle\": [\"M AH0\",\"P EH1 L\"],\n",
    "          \"toi\": [\"T W AA1\"],\n",
    "          \"brrrah\": [\"B R AA1\"],\n",
    "          \"brraaah\": [\"B R AA1\"],\n",
    "          \"drano\": ['D R EY1','N OW0'],\n",
    "          \"am\":[\"AE1 M\"],\n",
    "          \"diametric'ly\": [\"D AY2\",\"AH0\",\"M EH1\",\"T R IH0 K\",\"L IY0\"],\n",
    "          \"hypertone\": [\"HH AY1\",\"P ER0\",\"T OW2 N\"],\n",
    "          \"unawareness\": [\"AH2 N\",\"AH0\",\"W EH1 R\",\"N AH0 S\"],\n",
    "          \"assassinator\": [\"AH0\",\"S AE1\",\"S AH0\",\"N EY2 T\",\"AO0 R\"],\n",
    "          \"fiending\": [\"F IY1 N\",\"D IH0 NG\"],\n",
    "          \"dj\": [\"D IY1\",\"JH EY0\"],\n",
    "          \"ev'ry\": ['EH1', 'V R IY0'],\n",
    "          \"famished\": ['F AE M', 'IH SH T'],\n",
    "          \"blamin'\": [\"B L EY1\", \"M IH0 N\"],\n",
    "          \"corsets\": [\"K AO1 R\",\"S AH0 T S\"],\n",
    "          \"shittin'\": [\"SH IH1\", \"T IH0 NG\"],\n",
    "          \"hist'ry\": [\"HH IH1\",\"T R IY0\"],\n",
    "          \"onarchy\": [\"AA1\",\"N AA0 R\",\"K IY0\"],\n",
    "          \"knuckleheads\": [\"N AH1\", \"K AH0 L\", \"HH EH2 D Z\"],\n",
    "          \"parentis\": [\"P AH0\",\"R EH1 N\",\"T AH0 S\"],\n",
    "          \"cuz\": [\"K AH1 Z\"],\n",
    "          \"manumission\": [\"M AE1\",\"N Y UW0\",\"M IH2\",\"SH AH0 N\"],\n",
    "          \"hungriest\": [\"HH AH1 NG\", \"G R IY0\",\"AH0 S T\"],\n",
    "          \"unimpeachable\": [\"AH2 N\",\"IH0 M\",\"P IY1\",\"CH AH0\",\"B OW0 L\"],\n",
    "          \"impoverished\": [\"IH0 M\",\"P AA1\",\"V ER0\",\"IH0 SH T\"],\n",
    "          \"i'mma\": [\"AY1\",\"M AH0\"],\n",
    "          \"rakim\": [\"R AH0\",\"K IY1 M\"],\n",
    "          \"microphonist\": [\"M AY1\",\"K R AH0\",\"F OW2\",\"N AH0 S T\"],\n",
    "          \"dissed\": [\"D IH1 S T\"],\n",
    "          \"7\": [\"S EH1\",\"V AH0 N\"],\n",
    "          \"21\": [\"T W EH1 N\",\"T IY0\",\"W AH1 N\"],\n",
    "          \"fessing\": [\"F EH1\",\"S IH0 NG\"],\n",
    "          \"dissing\": [\"D IH1\",\"S IH0 NG\"],\n",
    "          \"marl\": [\"M AA1 R L\"],\n",
    "          \"blaow\": [\"B L AW1\"],\n",
    "          \"illest\": [\"IH1 L\", \"AH0 S T\"],\n",
    "          \"realest\": [\"R IY1 L\", \"AH0 S T\"],\n",
    "          \"50\": [\"F IH1 F\", \"T IY0\"],\n",
    "          \"underoo\": [\"AH1 N\",\"D ER0\",\"UW2 S\"],\n",
    "          \"'what's\": [\"W AH0 T S\"],\n",
    "          \"hoodie\": [\"HH UH1\",\"D IY0\"],\n",
    "          \"atch\": [\"AE1 CH\"],\n",
    "          \"niggas'll\": [\"N IH1\",\"G AH0 Z\",\"UH0 L\"],\n",
    "          \"nigga\": [\"N IH1\",\"G AH0\"],\n",
    "          \"niggaz\": [\"N IH1\",\"G AH0 Z\"],\n",
    "          \"dunny\": [\"D AH1\",\"N IY0\"],\n",
    "          \"fucker\": [\"F AH1\", \"K ER0\"],\n",
    "          \"fistfight\": [\"F IH1 S T\",\"F AY2 T\"],\n",
    "          \"motherfucking\": [\"M AH2\",\"DH ER0\",\"F AH1\",\"K IH0 NG\"],\n",
    "          \"motherfucker\": [\"M AH2\",\"DH ER0\",\"F AH1\",\"K EH0\"],\n",
    "          \"wonka\": [\"W AE1 N\", \"K AH0\"],\n",
    "          \"us\": [\"AH1 S\"],\n",
    "          \"thirstay\": [\"TH ER1\",\"S T EY0\"],\n",
    "          \"las\": [\"L AA1 S\"],\n",
    "          \"ovaltine\": [\"OW1\",\"V AH0 L\",\"T IY2 N\"],\n",
    "          \"'cedes\": [\"S EY1\",\"D IY0 Z\"],\n",
    "          \"papi\": [\"P AA1\",\"P IY0\"],\n",
    "          \"mobb\": [\"M AA1 B\"],\n",
    "          \"guccis\": [\"G UW1\",\"CH IY0 Z\"],\n",
    "          \"poom\": [\"P UW1 M\"],\n",
    "          \"pooms\": [\"P UW1 M Z\"],\n",
    "          \"boones\": [\"B UW1 N Z\"],\n",
    "          \"hoochies\": [\"HH UW1\",\"CH IY0\"],\n",
    "          \"coochies\": [\"K UW1\",\"CH IY0\"],\n",
    "          \"fugees\": [\"F UW1\",\"ZH IY0 Z\"],\n",
    "          \"sess\": [\"S EH1 S\"],\n",
    "          \"goretex\": [\"G AO1 R\",\"T EH0 K S\"],\n",
    "          \"'n'\": [\"EH0 N\"],\n",
    "          \"gon'\": [\"G AA1 N\"],\n",
    "          \"perfected\": [\"P ER0\",\"F EH1 K\",\"T IH0 D\"],\n",
    "          \"def\": [\"D EH1 F\"],\n",
    "          \"hero\": [\"H IY1\",\"R OW0\"],\n",
    "          \"mo'\": [\"M OW1\"],\n",
    "          \"nas\": [\"N AA1 S\"],\n",
    "          \"bama\": [\"B AE1\",\"M AH0\"],\n",
    "          \"audemars\": [\"AO1\",\"D EH0\",\"M AA0 R S\"],\n",
    "          \"rollies\": [\"R OW1\",\"L IY0 Z\"],\n",
    "          \"illuminati\": [\"IH0\",\"L UW1\",\"M IH0\",\"N AA0\",\"T IY0\"],\n",
    "          \"kanye\": [\"K AA1 N\",\"Y EY0\"],\n",
    "          \"cadien\": [\"K EY1\",\"D IY0\",\"AH0 N\"],\n",
    "          \"rhyme\": [\"R AY1 M\"],\n",
    "          \"rhymers\": [\"R AY1 M\",\"ER0 Z\"],\n",
    "          \"flossing\": [\"F L AO1 S\",\"IH0 NG\"],\n",
    "          \"tulip\": [\"T UW1\",\"L IH0 P\"],\n",
    "          \"illing\": [\"IH1 L\",\"IH0 NG\"],\n",
    "          \"'88\": [\"EY1\",\"T IY0\",\"EY2 T\"],\n",
    "          \"demeanour\": [\"D IH0\",\"M IY1\",\"N ER0\"],\n",
    "          \"jigga\": [\"JH IH1\",\"G AH0\"],\n",
    "          \"hova\": [\"HH OW1\",\"V AH0\"],\n",
    "          \"hov\": [\"HH OW1 V\"],\n",
    "          \"masqueradin\": [\"M AE2 S\",\"K ER0\",\"EY1\",\"D IH0 N\"],\n",
    "          \"atomically\": [\"AH0\",\"T AA1\",\"M AH0 K\",\"L IY0\"],\n",
    "          \"mockerie\": [\"M AA1\",\"K ER0\",\"IY0\"],\n",
    "          \"killa\": [\"K IH1\",\"L AH1\"],\n",
    "          \"beez\": [\"B IY1 Z\"],\n",
    "          \"shackling\": [\"SH AE1\",\"K AH0\", \"L IH0 NG\"],\n",
    "          \"shinobi\": [\"SH IH0\",\"N OW1\", \"B IY0\"],\n",
    "          \"y'all'll\": [\"Y AO2 L\",\"UH0 L\"],\n",
    "          \"baddest\": [\"B AE1\",\"D EH0 S T\"],\n",
    "          \"effervescence\": [\"EH2\",\"F ER0\",\"V EH1\",\"S EH0 N S\"],\n",
    "          \"16\": [\"S IH0 K S\",\"T IY1 N\"],\n",
    "          \"steelo\": [\"S T IY1\", \"L OW0\"],\n",
    "          \"ceelo\": [\"S IY1\", \"L OW0\"],\n",
    "          \"airplay\": [\"EH1 R\",\"P L EY0 N\"],\n",
    "          \"baseheads\": [\"B EY1 S\",\"HH EH2 D Z\"],\n",
    "          \"amps\": [\"AE1 M P S\"],\n",
    "          \"yellowback\": [\"Y EH1\",\"L OW0\",\"B AE2 K S\"],\n",
    "          \"realness\": [\"R IY1 L\",\"N AH0 S\"],\n",
    "          \"queensbridge\": [\"K W IY1 N Z\",\"B R IH2 JH\"],\n",
    "          \"nuff\": [\"N AH1 F\"],\n",
    "          \"squadron'll\": [\"S K W AA1\",\"D R AH0 N\",\"UH0 L\"],\n",
    "          \"bp\": [\"B IY1\",\"P IY2\"],\n",
    "          \"mañana\": [\"M AH0 N\",\"Y AA1\",\"N AA0\"],\n",
    "          \"manzana\": [\"M AH0 N\",\"Z AA1\",\"N AA0\"],\n",
    "          \"pana\": [\"P AA1\",\"N AA0\"],\n",
    "          \"slurping\": [\"S L ER1\",\"P IH0 NG\"],\n",
    "          \"diddly\": [\"D IH1 D\",\"L IY0\"],\n",
    "          \"loca\": [\"L OW1\",\"K AH0\"],\n",
    "          \"tocha\": [\"T OW1\",\"CH AH0\"],\n",
    "          \"pinga\": [\"P IH1 NG\", \"G AH0\"],\n",
    "          \"bolla\": [\"B OW1\",\"L AH0\"],\n",
    "          \"boriqua\": [\"B AO0 R\",\"IY1\",\"K W AH0\"],\n",
    "          \"'scuse\": [\"S K Y UW1 S\"],\n",
    "          \"cunt\": [\"K AH1 N T\"],\n",
    "          \"menage\": [\"M AY1\",\"N AA2 JH\"],\n",
    "          \"percenter\": [\"P ER0\",\"S EH1 N\",\"T ER0\"],\n",
    "          \"capers'll\": [\"K EY1\",\"P ER0 Z\",\"UH0 L\"],\n",
    "          \"bullethole\": [\"B UH1\",\"L AH0 T\",\"HH OW2 L\"],\n",
    "          \"peephole\": [\"P IY1 P\",\"HH OW2 L\"],\n",
    "          \"boardhead\": [\"B AO1 R D\",\"HH EH2 D\"],\n",
    "          \"1000\": [\"W AH1 N\",\"TH AW2\",\"Z AH0 N D\"],\n",
    "          \"lb\": [\"EH1 L\",\"B IY2\"],\n",
    "          \"punisher\": [\"P AH1\",\"N IH0\",\"SH ER0\"],\n",
    "          \"slaver\": [\"S L EY1\",\"V ER0\"],\n",
    "          \"ev'ryone\": [\"EH1\",\"V R IY0\",\"W AH2 N\"],\n",
    "          \"rochambeau\": [\"R OW1\",\"SH AE0 M\",\"B OW2\"],\n",
    "          \"ingenuitive\": [\"IH0 N\",\"JH AH0\",\"N UW1\",\"AH0\",\"T IH0 V\"],\n",
    "          \"outplanned\": [\"AW0 T\",\"P L AE1 N D\"],\n",
    "          \"giddyup\": [\"G IH1\",\"D IY0\",\"AH2 P\"]\n",
    "         }\n",
    "    return slang"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "code_folding": [
     0
    ],
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def clean_slang_dict(slang_dict):\n",
    "    \n",
    "    for key, value in slang_dict.items():\n",
    "        \n",
    "        phoneme_list = value\n",
    "        # Join all of the phonemes (previously seperated by commas into sylables) into one list\n",
    "        phoneme_list = ' '.join(phoneme_list)\n",
    "        \n",
    "        phoneme = phoneme_list.split()\n",
    "        slang_dict[key] = phoneme\n",
    "    \n",
    "    return slang_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     1
    ],
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "slang_dict = clean_slang_dict(get_slang_dict_dirty())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "code_folding": [
     0
    ],
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error '5'\n",
      "Error '5'\n",
      "Error 'yelawolf'\n",
      "Error 'drinkin'\n",
      "Error 'brauds'\n",
      "Error 'whistlin'\n",
      "Error 'slumerican'\n",
      "Error 'dopeman'\n",
      "Error 'choppin'\n",
      "Error 'bringin'\n",
      "Error 'hookless'\n",
      "Error 'didn'\n",
      "Error 'runnin'\n",
      "Used the slang dict on 16\n",
      "Error 'S IH0 K S'\n",
      "Used the slang dict on 7\n",
      "Error 'S EH1'\n",
      "Error '2'\n",
      "Error '87'\n",
      "Error is 17\n"
     ]
    }
   ],
   "source": [
    "def txt_to_phoneme_types(file):\n",
    "    sounds = []\n",
    "    error = 0\n",
    "    \n",
    "    with open(file, 'r') as f:\n",
    "        for line in f:\n",
    "\n",
    "            # Remove punctuation and tokenize\n",
    "            tokenizer = RegexpTokenizer(r'\\w+')\n",
    "            tokens = tokenizer.tokenize(line)\n",
    "            \n",
    "            # For every word, split it into phonemes \n",
    "            # Then, find out what type of sound that phoneme is and add it to an array\n",
    "            for word in tokens:\n",
    "                word = word.lower()\n",
    "\n",
    "                try:\n",
    "                    phonemes = arpabet[word][0]\n",
    "                    \n",
    "                    for p in phonemes:\n",
    "                        sound = phoneme_sound_dict(p)\n",
    "                        sounds.append(sound)\n",
    "\n",
    "                except Exception as e:\n",
    "                    try:\n",
    "                        phonemes = get_slang_phonemes(word)\n",
    "                        print('Used the slang dict on ' + word)\n",
    "                        for p in phonemes:\n",
    "                            sound = phoneme_sound_dict(p)\n",
    "                            sounds.append(sound)\n",
    "\n",
    "                    except Exception as e:\n",
    "                        print(\"Error \" + str(e))\n",
    "                        error += 1\n",
    "                        sounds.append('error')\n",
    "\n",
    "                sounds.append('space')\n",
    "                \n",
    "            sounds.append('new line')\n",
    "\n",
    "        print('Error is ' + str(error))\n",
    "        return sounds\n",
    "       \n",
    "sounds = txt_to_phoneme_types('Radioactive-Introduction_Yelawolf.txt')\n",
    "\n",
    "#txt_to_phoneme_types('Radioactive-Introduction_Yelawolf.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "code_folding": [
     1
    ],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This makes a similarity matrix for +/- 3 sounds before and after a sounds\n",
    "def make_sim_matrix(phonemes_list):\n",
    "    \n",
    "    sim_matrix = [[]]\n",
    "    \n",
    "    for p in phonemes_list:\n",
    "        index = phonemes_list.get_loc[p]\n",
    "        if (index > 3) and (index < (len(phonemes) - 3)):\n",
    "            sim_matrix[index, index - 3] = phoneme_similarity(phonemes[index], phonemes[index - 3])\n",
    "            sim_matrix[index, index - 2] = phoneme_similarity(phonemes[index], phonemes[index - 2])\n",
    "            sim_matrix[index, index - 1] = phoneme_similarity(phonemes[index], phonemes[index - 1])\n",
    "            sim_matrix[index, index] = phoneme_similarity(phonemes[index], phonemes[index])\n",
    "            sim_matrix[index, index + 1] = phoneme_similarity(phonemes[index], phonemes[index + 1])\n",
    "            sim_matrix[index, index + 2] = phoneme_similarity(phonemes[index], phonemes[index + 2])\n",
    "            sim_matrix[index, index + 3] = phoneme_similarity(phonemes[index], phonemes[index + 3])\n",
    "\n",
    "        elif (index == 1):\n",
    "            sim_matrix[index, index] = phoneme_similarity(phonemes[index], phonemes[index])\n",
    "            sim_matrix[index, index + 1] = phoneme_similarity(phonemes[index], phonemes[index + 1])\n",
    "            sim_matrix[index, index + 2] = phoneme_similarity(phonemes[index], phonemes[index + 2])\n",
    "            sim_matrix[index, index + 3] = phoneme_similarity(phonemes[index], phonemes[index + 3])\n",
    "        \n",
    "        elif (index == 2):\n",
    "            sim_matrix[index, index - 1] = phoneme_similarity(phonemes[index], phonemes[index - 1])\n",
    "            sim_matrix[index, index] = phoneme_similarity(phonemes[index], phonemes[index])\n",
    "            sim_matrix[index, index + 1] = phoneme_similarity(phonemes[index], phonemes[index + 1])\n",
    "            sim_matrix[index, index + 2] = phoneme_similarity(phonemes[index], phonemes[index + 2])\n",
    "            sim_matrix[index, index + 3] = phoneme_similarity(phonemes[index], phonemes[index + 3])\n",
    "        \n",
    "        elif (index == 3):\n",
    "            sim_matrix[index, index - 2] = phoneme_similarity(phonemes[index], phonemes[index - 2])\n",
    "            sim_matrix[index, index - 1] = phoneme_similarity(phonemes[index], phonemes[index - 1])\n",
    "            sim_matrix[index, index] = phoneme_similarity(phonemes[index], phonemes[index])\n",
    "            sim_matrix[index, index + 1] = phoneme_similarity(phonemes[index], phonemes[index + 1])\n",
    "            sim_matrix[index, index + 2] = phoneme_similarity(phonemes[index], phonemes[index + 2])\n",
    "            sim_matrix[index, index + 3] = phoneme_similarity(phonemes[index], phonemes[index + 3])\n",
    "\n",
    "        elif (index == len(phonmes) - 3):\n",
    "            sim_matrix[index, index - 3] = phoneme_similarity(phonemes[index], phonemes[index - 3])\n",
    "            sim_matrix[index, index - 2] = phoneme_similarity(phonemes[index], phonemes[index - 2])\n",
    "            sim_matrix[index, index - 1] = phoneme_similarity(phonemes[index], phonemes[index - 1])\n",
    "            sim_matrix[index, index] = phoneme_similarity(phonemes[index], phonemes[index])\n",
    "            sim_matrix[index, index + 1] = phoneme_similarity(phonemes[index], phonemes[index + 1])\n",
    "            sim_matrix[index, index + 2] = phoneme_similarity(phonemes[index], phonemes[index + 2])\n",
    "    \n",
    "\n",
    "        elif (index == len(phonmes) - 2):\n",
    "            sim_matrix[index, index - 3] = phoneme_similarity(phonemes[index], phonemes[index - 3])\n",
    "            sim_matrix[index, index - 2] = phoneme_similarity(phonemes[index], phonemes[index - 2])\n",
    "            sim_matrix[index, index - 1] = phoneme_similarity(phonemes[index], phonemes[index - 1])\n",
    "            sim_matrix[index, index] = phoneme_similarity(phonemes[index], phonemes[index])\n",
    "            sim_matrix[index, index + 1] = phoneme_similarity(phonemes[index], phonemes[index + 1])\n",
    "        \n",
    "\n",
    "        elif (index == len(phonmes) - 1):\n",
    "            sim_matrix[index, index - 3] = phoneme_similarity(phonemes[index], phonemes[index - 3])\n",
    "            sim_matrix[index, index - 2] = phoneme_similarity(phonemes[index], phonemes[index - 2])\n",
    "            sim_matrix[index, index - 1] = phoneme_similarity(phonemes[index], phonemes[index - 1])\n",
    "            sim_matrix[index, index] = phoneme_similarity(phonemes[index], phonemes[index])\n",
    "    \n",
    "            \n",
    "        return sim_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "code_folding": [
     0,
     9,
     13
    ],
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (<ipython-input-119-fa10e3e169d0>, line 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-119-fa10e3e169d0>\"\u001b[0;36m, line \u001b[0;32m4\u001b[0m\n\u001b[0;31m    p = nltk.sound_type(p)\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "def phoneme_similarity(p, p1):\n",
    "    \n",
    "    # What type of sounds is the phononeme\n",
    "    p = nltk.sound_type(p)\n",
    "    p1 = nltk.sound_type(p)\n",
    "    \n",
    "    # Note that phoneme_similarity(p, p1) == phoneme_similarity(p1, p)\n",
    "    # That is phoneme_similarity is symmetric\n",
    "    pass\n",
    "'''\n",
    "    if p == p1:\n",
    "        sim = 1\n",
    "        \n",
    "    elif p == 'fricative':\n",
    "        if p1 == 'affricate':\n",
    "            sim = \n",
    "        if p1 == 'vowel':\n",
    "            sim = \n",
    "        if p1 == 'semivowel':\n",
    "            sim = \n",
    "        if p1 == 'nasal':\n",
    "            sim = \n",
    "        if p1 == 'liquid':\n",
    "            sim = \n",
    "        if p1 == 'aspirate':\n",
    "            sim\n",
    "        if p1 == 'stop':\n",
    "            sim = \n",
    "            \n",
    "    elif p == 'affricate':\n",
    "        if p1 == 'vowel':\n",
    "            sim = \n",
    "        if p1 == 'semivowel':\n",
    "            sim = \n",
    "        if p1 == 'nasal':\n",
    "            sim = \n",
    "        if p1 == 'liquid':\n",
    "            sim = \n",
    "        if p1 == 'aspirate':\n",
    "            sim\n",
    "        if p1 == 'stop':\n",
    "            sim = \n",
    "\n",
    "            \n",
    "    elif p == 'vowel':\n",
    "        if p1 == 'semivowel':\n",
    "            sim = \n",
    "        if p1 == 'nasal':\n",
    "            sim = \n",
    "        if p1 == 'liquid':\n",
    "            sim = \n",
    "        if p1 == 'aspirate':\n",
    "            sim\n",
    "        if p1 == 'stop':\n",
    "            sim = \n",
    "            \n",
    "    elif p == 'semivowel':\n",
    "        if p1 == 'nasal':\n",
    "            sim = \n",
    "        if p1 == 'liquid':\n",
    "            sim = \n",
    "        if p1 == 'aspirate':\n",
    "            sim\n",
    "        if p1 == 'stop':\n",
    "            sim = \n",
    "            \n",
    "    elif p == 'nasal':\n",
    "        if p1 == 'liquid':\n",
    "            sim = \n",
    "        if p1 == 'aspirate':\n",
    "            sim\n",
    "        if p1 == 'stop':\n",
    "            sim = \n",
    "            \n",
    "    elif p == 'liquid':\n",
    "        if p1 == 'aspirate':\n",
    "            sim\n",
    "        if p1 == 'stop':\n",
    "            sim = \n",
    "            \n",
    "    elif p == 'aspirate':\n",
    "        if p1 == 'stop':\n",
    "            sim = \n",
    "    \n",
    "        \n",
    "        \n",
    "    return sim\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {
    "code_folding": [],
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15, 154)\n",
      "[[ 0.4   0.99  0.   ...,  0.    0.    0.2 ]\n",
      " [ 0.99  0.    0.99 ...,  0.    0.99  0.6 ]\n",
      " [ 0.6   0.4   0.99 ...,  0.6   0.6   0.6 ]\n",
      " ..., \n",
      " [ 0.99  0.4   0.4  ...,  0.2   0.2   0.  ]\n",
      " [ 0.99  0.    0.99 ...,  0.99  0.6   0.99]\n",
      " [ 0.6   0.6   0.99 ...,  0.2   0.2   0.2 ]]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAABDCAYAAACMa/7yAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nrkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a12094ef0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def visualize(sounds):\n",
    "    m = np.matrix([1]*len(sounds), dtype=float).T\n",
    "    count = 0\n",
    "    \n",
    "    for sound in sounds:\n",
    "        #print(sound)\n",
    "        \n",
    "        if sound is 'fricative':\n",
    "            val = .2\n",
    "        elif sound is 'affricate':\n",
    "            val = .3 \n",
    "        elif sound is 'vowel':\n",
    "            val = .99\n",
    "        elif sound is 'semivowel':\n",
    "            val = .8\n",
    "        elif sound is 'nasal':\n",
    "            val = .6\n",
    "        elif sound is 'liquid':\n",
    "            val = .4\n",
    "        elif sound is 'aspirate':\n",
    "            val = .7\n",
    "        elif sound is 'stop':\n",
    "            val = 0\n",
    "            \n",
    "        #print(val)\n",
    "        \n",
    "        #print('The count and val are {} and {}'.format(count, val))\n",
    "        m.put(count, val)\n",
    "        #m.put(.9, 12)\n",
    "        count +=  1\n",
    "    \n",
    "    m = np.reshape(m, (15,154))\n",
    "    print(m.shape)\n",
    "    print(m)\n",
    "\n",
    "    data = np.random.rand(10, 10)\n",
    "    #x = np.linspace(0, 3 * np.pi, 500)\n",
    "    #y = np.sin(x)\n",
    "\n",
    "    # create discrete colormap\n",
    "    cmap = colors.ListedColormap(['red', 'blue', 'cyan', 'green', 'violet', 'purple', 'gold', 'orange'])\n",
    "    bounds = [0, .21, .31, .41, .51, .61, .71, .81, 1]\n",
    "    #bounds = ['fricative', 'stop', 'affricative']\n",
    "    norm = colors.BoundaryNorm(bounds, cmap.N)\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.imshow(m, cmap=cmap, norm=norm)\n",
    "\n",
    "    # draw gridlines\n",
    "    ax.grid(which='major', axis='both', linestyle='-', color='k', linewidth=1)\n",
    "    #ax.set_xticks(np.arange(-.5, 10, 1));\n",
    "    #ax.set_yticks(np.arange(-.5, 10, 1));\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "visualize(sounds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    sounds = txt_to_phoneme_types('Radioactive-Introduction_Yelawolf.txt')\n",
    "    #sim_matrix = make_sim_matrix(sounds)\n",
    "    vizualize(sounds)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "code_folding": [
     0
    ],
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'semivowel'"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def phoneme_sound_dict(key):\n",
    "    phon_dict = {\n",
    "    'AA':'vowel',\n",
    "    'AA0':'vowel',\n",
    "    'AA1':'vowel',\n",
    "    'AA2':'vowel',\n",
    "    'AE':'vowel',\n",
    "    'AE0':'vowel',\n",
    "    'AE1':'vowel',\n",
    "    'AE2':'vowel',\n",
    "    'AH':'vowel',\n",
    "    'AH0':'vowel',\n",
    "    'AH1':'vowel',\n",
    "    'AH2':'vowel',\n",
    "    'AO':'vowel',\n",
    "    'AO0':'vowel',\n",
    "    'AO1':'vowel',\n",
    "    'AO2':'vowel',\n",
    "    'AW':'vowel',\n",
    "    'AW0':'vowel',\n",
    "    'AW1':'vowel',\n",
    "    'AW2':'vowel',\n",
    "    'AY':'vowel',\n",
    "    'AY0':'vowel',\n",
    "    'AY1':'vowel',\n",
    "    'AY2':'vowel',\n",
    "    'B':'stop',\n",
    "    'CH':'affricate',\n",
    "    'D':'stop',\n",
    "    'DH':'fricative',\n",
    "    'EH':'vowel',\n",
    "    'EH0':'vowel',\n",
    "    'EH1':'vowel',\n",
    "    'EH2':'vowel',\n",
    "    'ER':'vowel',\n",
    "    'ER0':'vowel',\n",
    "    'ER1':'vowel',\n",
    "    'ER2':'vowel',\n",
    "    'EY':'vowel',\n",
    "    'EY0':'vowel',\n",
    "    'EY1':'vowel',\n",
    "    'EY2':'vowel',\n",
    "    'F':'fricative',\n",
    "    'G':'stop',\n",
    "    'HH':'aspirate',\n",
    "    'IH':'vowel',\n",
    "    'IH0':'vowel',\n",
    "    'IH1':'vowel',\n",
    "    'IH2':'vowel',\n",
    "    'IY':'vowel',\n",
    "    'IY0':'vowel',\n",
    "    'IY1':'vowel',\n",
    "    'IY2':'vowel',\n",
    "    'JH':'affricate',\n",
    "    'K':'stop',\n",
    "    'L':'liquid',\n",
    "    'M':'nasal',\n",
    "    'N':'nasal',\n",
    "    'NG':'nasal',\n",
    "    'OW':'vowel',\n",
    "    'OW0':'vowel',\n",
    "    'OW1':'vowel',\n",
    "    'OW2':'vowel',\n",
    "    'OY':'vowel',\n",
    "    'OY0':'vowel',\n",
    "    'OY1':'vowel',\n",
    "    'OY2':'vowel',\n",
    "    'P':'stop',\n",
    "    'R':'liquid',\n",
    "    'S':'fricative',\n",
    "    'SH':'fricative',\n",
    "    'T': 'stop',\n",
    "    'TH':'fricative',\n",
    "    'UH':'vowel',\n",
    "    'UH0':'vowel',\n",
    "    'UH1':'vowel',\n",
    "    'UH2':'vowel',\n",
    "    'UW':'vowel',\n",
    "    'UW0':'vowel',\n",
    "    'UW1':'vowel',\n",
    "    'UW2':'vowel',\n",
    "    'V':'fricative',\n",
    "    'W' : 'semivowel',\n",
    "    'Y':'semivowel',\n",
    "    'Z':'fricative',\n",
    "    'ZH':'fricative',\n",
    "    }\n",
    "    \n",
    "    return phon_dict[key]\n",
    "\n",
    "#phoneme_sound_dict('Y')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OLD CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "code_folding": [
     0
    ],
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (<ipython-input-10-5501ab68f4f2>, line 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-10-5501ab68f4f2>\"\u001b[0;36m, line \u001b[0;32m4\u001b[0m\n\u001b[0;31m    p = nltk.sound_type(p)\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "#def phoneme_similarity_old(p, p1):\n",
    "    \n",
    "    # What type of sounds is the phononeme\n",
    "    p = nltk.sound_type(p)\n",
    "    p1 = nltk.sound_type(p)\n",
    "    \n",
    "    if p == p1:\n",
    "        sim = 1\n",
    "        \n",
    "    elif p == 'fricative':\n",
    "        if p1 == 'affricate':\n",
    "            sim = \n",
    "        if p1 == 'vowel':\n",
    "            sim = \n",
    "        if p1 == 'semivowel':\n",
    "            sim = \n",
    "        if p1 == 'nasal':\n",
    "            sim = \n",
    "        if p1 == 'liquid':\n",
    "            sim = \n",
    "        if p1 == 'aspirate':\n",
    "            sim\n",
    "        if p1 == 'stop':\n",
    "            sim = \n",
    "            \n",
    "    elif p == 'affricate':\n",
    "        if p1 == 'fricative':\n",
    "            sim = \n",
    "        if p1 == 'vowel':\n",
    "            sim = \n",
    "        if p1 == 'semivowel':\n",
    "            sim = \n",
    "        if p1 == 'nasal':\n",
    "            sim = \n",
    "        if p1 == 'liquid':\n",
    "            sim = \n",
    "        if p1 == 'aspirate':\n",
    "            sim\n",
    "        if p1 == 'stop':\n",
    "            sim = \n",
    "\n",
    "            \n",
    "    elif p == 'vowel':\n",
    "        if p1 == 'fricative':\n",
    "            sim = \n",
    "        if p1 == 'affricate':\n",
    "            sim = \n",
    "        if p1 == 'semivowel':\n",
    "            sim = \n",
    "        if p1 == 'nasal':\n",
    "            sim = \n",
    "        if p1 == 'liquid':\n",
    "            sim = \n",
    "        if p1 == 'aspirate':\n",
    "            sim\n",
    "        if p1 == 'stop':\n",
    "            sim = \n",
    "            \n",
    "    elif p == 'semivowel':\n",
    "        if p1 == 'fricative':\n",
    "            sim = \n",
    "        if p1 == 'affricate':\n",
    "            sim = \n",
    "        if p1 == 'vowel':\n",
    "            sim = \n",
    "        if p1 == 'nasal':\n",
    "            sim = \n",
    "        if p1 == 'liquid':\n",
    "            sim = \n",
    "        if p1 == 'aspirate':\n",
    "            sim\n",
    "        if p1 == 'stop':\n",
    "            sim = \n",
    "            \n",
    "    elif p == 'nasal':\n",
    "        if p1 == 'fricative':\n",
    "            sim = \n",
    "        if p1 == 'affricate':\n",
    "            sim = \n",
    "        if p1 == 'vowel':\n",
    "            sim = \n",
    "        if p1 == 'semivowel':\n",
    "            sim = \n",
    "        if p1 == 'liquid':\n",
    "            sim = \n",
    "        if p1 == 'aspirate':\n",
    "            sim\n",
    "        if p1 == 'stop':\n",
    "            sim = \n",
    "            \n",
    "    elif p == 'liquid':\n",
    "        if p1 == 'fricative':\n",
    "            sim = \n",
    "        if p1 == 'affricate':\n",
    "            sim = \n",
    "        if p1 == 'vowel':\n",
    "            sim = \n",
    "        if p1 == 'semivowel':\n",
    "            sim = \n",
    "        if p1 == 'nasal':\n",
    "            sim = \n",
    "        if p1 == 'aspirate':\n",
    "            sim\n",
    "        if p1 == 'stop':\n",
    "            sim = \n",
    "            \n",
    "    elif p == 'aspirate':\n",
    "        if p1 == 'fricative':\n",
    "            sim = \n",
    "        if p1 == 'affricate':\n",
    "            sim = \n",
    "        if p1 == 'vowel':\n",
    "            sim = \n",
    "        if p1 == 'semivowel':\n",
    "            sim = \n",
    "        if p1 == 'nasal':\n",
    "            sim = \n",
    "        if p1 == 'liquid':\n",
    "            sim = \n",
    "        if p1 == 'stop':\n",
    "            sim = \n",
    "            \n",
    "    elif p == 'stop':\n",
    "        if p1 == 'fricative':\n",
    "            sim = \n",
    "        if p1 == 'affricate':\n",
    "            sim = \n",
    "        if p1 == 'vowel':\n",
    "            sim = \n",
    "        if p1 == 'semivowel':\n",
    "            sim = \n",
    "        if p1 == 'nasal':\n",
    "            sim = \n",
    "        if p1 == 'liquid':\n",
    "            sim = \n",
    "        if p1 == 'aspirate':\n",
    "            sim\n",
    "    \n",
    "        \n",
    "        \n",
    "    return sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "code_folding": [
     0
    ],
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['D R EY1', 'N OW0']"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_dirty_slang_phonemes(key):\n",
    "    slang = {\n",
    "        \"prob'ly\":['P R AA1', 'B L IY0'],\n",
    "          \"practic'ly\": ['P R AE1 K','T AH0 K','L IY0'],\n",
    "          \"acquiescent\":  [\"AE2\",\"K W IY0\",\"EH1\",\"S AH0 N T\"],\n",
    "          \"mediocrities\": [\"M IY2\",\"D IY0\",\"AA1\",\"K R AH0\",\"T IY0 Z\"],\n",
    "          \"casse\": [\"K AH0 S\"],\n",
    "          \"bonsoir\": [\"B AA0 N\",\"S W AA1\"],\n",
    "          \"m'appelle\": [\"M AH0\",\"P EH1 L\"],\n",
    "          \"toi\": [\"T W AA1\"],\n",
    "          \"brrrah\": [\"B R AA1\"],\n",
    "          \"brraaah\": [\"B R AA1\"],\n",
    "          \"drano\": ['D R EY1','N OW0'],\n",
    "          \"am\":[\"AE1 M\"],\n",
    "          \"diametric'ly\": [\"D AY2\",\"AH0\",\"M EH1\",\"T R IH0 K\",\"L IY0\"],\n",
    "          \"hypertone\": [\"HH AY1\",\"P ER0\",\"T OW2 N\"],\n",
    "          \"unawareness\": [\"AH2 N\",\"AH0\",\"W EH1 R\",\"N AH0 S\"],\n",
    "          \"assassinator\": [\"AH0\",\"S AE1\",\"S AH0\",\"N EY2 T\",\"AO0 R\"],\n",
    "          \"fiending\": [\"F IY1 N\",\"D IH0 NG\"],\n",
    "          \"dj\": [\"D IY1\",\"JH EY0\"],\n",
    "          \"ev'ry\": ['EH1', 'V R IY0'],\n",
    "          \"famished\": ['F AE M', 'IH SH T'],\n",
    "          \"blamin'\": [\"B L EY1\", \"M IH0 N\"],\n",
    "          \"corsets\": [\"K AO1 R\",\"S AH0 T S\"],\n",
    "          \"shittin'\": [\"SH IH1\", \"T IH0 NG\"],\n",
    "          \"hist'ry\": [\"HH IH1\",\"T R IY0\"],\n",
    "          \"onarchy\": [\"AA1\",\"N AA0 R\",\"K IY0\"],\n",
    "          \"knuckleheads\": [\"N AH1\", \"K AH0 L\", \"HH EH2 D Z\"],\n",
    "          \"parentis\": [\"P AH0\",\"R EH1 N\",\"T AH0 S\"],\n",
    "          \"cuz\": [\"K AH1 Z\"],\n",
    "          \"manumission\": [\"M AE1\",\"N Y UW0\",\"M IH2\",\"SH AH0 N\"],\n",
    "          \"hungriest\": [\"HH AH1 NG\", \"G R IY0\",\"AH0 S T\"],\n",
    "          \"unimpeachable\": [\"AH2 N\",\"IH0 M\",\"P IY1\",\"CH AH0\",\"B OW0 L\"],\n",
    "          \"impoverished\": [\"IH0 M\",\"P AA1\",\"V ER0\",\"IH0 SH T\"],\n",
    "          \"i'mma\": [\"AY1\",\"M AH0\"],\n",
    "          \"rakim\": [\"R AH0\",\"K IY1 M\"],\n",
    "          \"microphonist\": [\"M AY1\",\"K R AH0\",\"F OW2\",\"N AH0 S T\"],\n",
    "          \"dissed\": [\"D IH1 S T\"],\n",
    "          \"7\": [\"S EH1\",\"V AH0 N\"],\n",
    "          \"21\": [\"T W EH1 N\",\"T IY0\",\"W AH1 N\"],\n",
    "          \"fessing\": [\"F EH1\",\"S IH0 NG\"],\n",
    "          \"dissing\": [\"D IH1\",\"S IH0 NG\"],\n",
    "          \"marl\": [\"M AA1 R L\"],\n",
    "          \"blaow\": [\"B L AW1\"],\n",
    "          \"illest\": [\"IH1 L\", \"AH0 S T\"],\n",
    "          \"realest\": [\"R IY1 L\", \"AH0 S T\"],\n",
    "          \"50\": [\"F IH1 F\", \"T IY0\"],\n",
    "          \"underoo\": [\"AH1 N\",\"D ER0\",\"UW2 S\"],\n",
    "          \"'what's\": [\"W AH0 T S\"],\n",
    "          \"hoodie\": [\"HH UH1\",\"D IY0\"],\n",
    "          \"atch\": [\"AE1 CH\"],\n",
    "          \"niggas'll\": [\"N IH1\",\"G AH0 Z\",\"UH0 L\"],\n",
    "          \"nigga\": [\"N IH1\",\"G AH0\"],\n",
    "          \"niggaz\": [\"N IH1\",\"G AH0 Z\"],\n",
    "          \"dunny\": [\"D AH1\",\"N IY0\"],\n",
    "          \"fucker\": [\"F AH1\", \"K ER0\"],\n",
    "          \"fistfight\": [\"F IH1 S T\",\"F AY2 T\"],\n",
    "          \"motherfucking\": [\"M AH2\",\"DH ER0\",\"F AH1\",\"K IH0 NG\"],\n",
    "          \"motherfucker\": [\"M AH2\",\"DH ER0\",\"F AH1\",\"K EH0\"],\n",
    "          \"wonka\": [\"W AE1 N\", \"K AH0\"],\n",
    "          \"us\": [\"AH1 S\"],\n",
    "          \"thirstay\": [\"TH ER1\",\"S T EY0\"],\n",
    "          \"las\": [\"L AA1 S\"],\n",
    "          \"ovaltine\": [\"OW1\",\"V AH0 L\",\"T IY2 N\"],\n",
    "          \"'cedes\": [\"S EY1\",\"D IY0 Z\"],\n",
    "          \"papi\": [\"P AA1\",\"P IY0\"],\n",
    "          \"mobb\": [\"M AA1 B\"],\n",
    "          \"guccis\": [\"G UW1\",\"CH IY0 Z\"],\n",
    "          \"poom\": [\"P UW1 M\"],\n",
    "          \"pooms\": [\"P UW1 M Z\"],\n",
    "          \"boones\": [\"B UW1 N Z\"],\n",
    "          \"hoochies\": [\"HH UW1\",\"CH IY0\"],\n",
    "          \"coochies\": [\"K UW1\",\"CH IY0\"],\n",
    "          \"fugees\": [\"F UW1\",\"ZH IY0 Z\"],\n",
    "          \"sess\": [\"S EH1 S\"],\n",
    "          \"goretex\": [\"G AO1 R\",\"T EH0 K S\"],\n",
    "          \"'n'\": [\"EH0 N\"],\n",
    "          \"gon'\": [\"G AA1 N\"],\n",
    "          \"perfected\": [\"P ER0\",\"F EH1 K\",\"T IH0 D\"],\n",
    "          \"def\": [\"D EH1 F\"],\n",
    "          \"hero\": [\"H IY1\",\"R OW0\"],\n",
    "          \"mo'\": [\"M OW1\"],\n",
    "          \"nas\": [\"N AA1 S\"],\n",
    "          \"bama\": [\"B AE1\",\"M AH0\"],\n",
    "          \"audemars\": [\"AO1\",\"D EH0\",\"M AA0 R S\"],\n",
    "          \"rollies\": [\"R OW1\",\"L IY0 Z\"],\n",
    "          \"illuminati\": [\"IH0\",\"L UW1\",\"M IH0\",\"N AA0\",\"T IY0\"],\n",
    "          \"kanye\": [\"K AA1 N\",\"Y EY0\"],\n",
    "          \"cadien\": [\"K EY1\",\"D IY0\",\"AH0 N\"],\n",
    "          \"rhyme\": [\"R AY1 M\"],\n",
    "          \"rhymers\": [\"R AY1 M\",\"ER0 Z\"],\n",
    "          \"flossing\": [\"F L AO1 S\",\"IH0 NG\"],\n",
    "          \"tulip\": [\"T UW1\",\"L IH0 P\"],\n",
    "          \"illing\": [\"IH1 L\",\"IH0 NG\"],\n",
    "          \"'88\": [\"EY1\",\"T IY0\",\"EY2 T\"],\n",
    "          \"demeanour\": [\"D IH0\",\"M IY1\",\"N ER0\"],\n",
    "          \"jigga\": [\"JH IH1\",\"G AH0\"],\n",
    "          \"hova\": [\"HH OW1\",\"V AH0\"],\n",
    "          \"hov\": [\"HH OW1 V\"],\n",
    "          \"masqueradin\": [\"M AE2 S\",\"K ER0\",\"EY1\",\"D IH0 N\"],\n",
    "          \"atomically\": [\"AH0\",\"T AA1\",\"M AH0 K\",\"L IY0\"],\n",
    "          \"mockerie\": [\"M AA1\",\"K ER0\",\"IY0\"],\n",
    "          \"killa\": [\"K IH1\",\"L AH1\"],\n",
    "          \"beez\": [\"B IY1 Z\"],\n",
    "          \"shackling\": [\"SH AE1\",\"K AH0\", \"L IH0 NG\"],\n",
    "          \"shinobi\": [\"SH IH0\",\"N OW1\", \"B IY0\"],\n",
    "          \"y'all'll\": [\"Y AO2 L\",\"UH0 L\"],\n",
    "          \"baddest\": [\"B AE1\",\"D EH0 S T\"],\n",
    "          \"effervescence\": [\"EH2\",\"F ER0\",\"V EH1\",\"S EH0 N S\"],\n",
    "          \"16\": [\"S IH0 K S\",\"T IY1 N\"],\n",
    "          \"steelo\": [\"S T IY1\", \"L OW0\"],\n",
    "          \"ceelo\": [\"S IY1\", \"L OW0\"],\n",
    "          \"airplay\": [\"EH1 R\",\"P L EY0 N\"],\n",
    "          \"baseheads\": [\"B EY1 S\",\"HH EH2 D Z\"],\n",
    "          \"amps\": [\"AE1 M P S\"],\n",
    "          \"yellowback\": [\"Y EH1\",\"L OW0\",\"B AE2 K S\"],\n",
    "          \"realness\": [\"R IY1 L\",\"N AH0 S\"],\n",
    "          \"queensbridge\": [\"K W IY1 N Z\",\"B R IH2 JH\"],\n",
    "          \"nuff\": [\"N AH1 F\"],\n",
    "          \"squadron'll\": [\"S K W AA1\",\"D R AH0 N\",\"UH0 L\"],\n",
    "          \"bp\": [\"B IY1\",\"P IY2\"],\n",
    "          \"mañana\": [\"M AH0 N\",\"Y AA1\",\"N AA0\"],\n",
    "          \"manzana\": [\"M AH0 N\",\"Z AA1\",\"N AA0\"],\n",
    "          \"pana\": [\"P AA1\",\"N AA0\"],\n",
    "          \"slurping\": [\"S L ER1\",\"P IH0 NG\"],\n",
    "          \"diddly\": [\"D IH1 D\",\"L IY0\"],\n",
    "          \"loca\": [\"L OW1\",\"K AH0\"],\n",
    "          \"tocha\": [\"T OW1\",\"CH AH0\"],\n",
    "          \"pinga\": [\"P IH1 NG\", \"G AH0\"],\n",
    "          \"bolla\": [\"B OW1\",\"L AH0\"],\n",
    "          \"boriqua\": [\"B AO0 R\",\"IY1\",\"K W AH0\"],\n",
    "          \"'scuse\": [\"S K Y UW1 S\"],\n",
    "          \"cunt\": [\"K AH1 N T\"],\n",
    "          \"menage\": [\"M AY1\",\"N AA2 JH\"],\n",
    "          \"percenter\": [\"P ER0\",\"S EH1 N\",\"T ER0\"],\n",
    "          \"capers'll\": [\"K EY1\",\"P ER0 Z\",\"UH0 L\"],\n",
    "          \"bullethole\": [\"B UH1\",\"L AH0 T\",\"HH OW2 L\"],\n",
    "          \"peephole\": [\"P IY1 P\",\"HH OW2 L\"],\n",
    "          \"boardhead\": [\"B AO1 R D\",\"HH EH2 D\"],\n",
    "          \"1000\": [\"W AH1 N\",\"TH AW2\",\"Z AH0 N D\"],\n",
    "          \"lb\": [\"EH1 L\",\"B IY2\"],\n",
    "          \"punisher\": [\"P AH1\",\"N IH0\",\"SH ER0\"],\n",
    "          \"slaver\": [\"S L EY1\",\"V ER0\"],\n",
    "          \"ev'ryone\": [\"EH1\",\"V R IY0\",\"W AH2 N\"],\n",
    "          \"rochambeau\": [\"R OW1\",\"SH AE0 M\",\"B OW2\"],\n",
    "          \"ingenuitive\": [\"IH0 N\",\"JH AH0\",\"N UW1\",\"AH0\",\"T IH0 V\"],\n",
    "          \"outplanned\": [\"AW0 T\",\"P L AE1 N D\"],\n",
    "          \"giddyup\": [\"G IH1\",\"D IY0\",\"AH2 P\"]\n",
    "         }\n",
    "    return slang[key]\n",
    "\n",
    "get_dirty_slang_phonemes('drano')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#def make_slang_dict(slang_dict):\n",
    "    #slang = slang_dict\n",
    "    #return slang\n",
    "\n",
    "#make_slang_dict(slang_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:anaconda]",
   "language": "python",
   "name": "conda-env-anaconda-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
